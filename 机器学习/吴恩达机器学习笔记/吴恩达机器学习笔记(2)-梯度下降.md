通过前面的文章我们现在已经有了假设函数$h_\theta(x)$并知道如何度量这个函数与数据的符合程度，即代价函数$J(\theta_0,\theta_1)$取得最小值。那么现在要做的，就是如何去预估这个假设函数的参数来使得我们的函数更加符合实际数据。
如下图是代价函数$J(\theta_0,\theta_1)$的图像，x轴为$\theta_0$,y轴为$\theta_1$,z轴为$J$。 

![][1]

我们知道，要取得代价函数最小值那就是这个函数图像的最底部的值。如图我们需要一步步的移动直到找到最底部的那个点。

![][2]

要求的这个最小值，我们需要对代价函数求导，由于切线的斜率即是这个函数在该点的导数，可以提供每一步移动的方向。我们根据这个方向一步步的进行梯度下降，而每一步移动的大小这里用参数$\alpha$表示，被称作学习速率(learning rate)。图中的星星之间的距离就是由参数$\alpha$决定，方向则是$J(\theta_0,\theta_1)$的偏导数来决定。从不同的点起始，最终得到的重点可能也不一样，途中展示的就是两个不同的起点而导致不同的终点。

综上，梯度下降的算法就是不断的迭代直到收敛：

![][3]

其中$j=0,1$，表示我们当前的特征索引号。(":="这个符号表示赋值，若写作"="表示一个判断为真的声明。)注意，在每一次迭代中，我们是同步更新参数$\theta_0,\theta_1$，意思是，实现方法时，我们应该先计算公式右边的部分，通过计算得出$\theta_0,\theta_1$的值，然后同步更新$\theta_0,\theta_1$。举个例子，我假设：
$$
temp0 := \theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)
$$
$$
temp1 := \theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)
$$
然后计算出公式右边的部分，计算出$\theta_0,\theta_1$的值，然后同时更新$\theta_0,\theta_1$：
$$
\theta_0 := temp0
$$
$$
\theta_1 := temp1
$$
与此相反，下面就是错误的范例，这个错误的方法中我们计算temp0，然后更新θ0，再计算temp1，最后将temp1赋给θ1。这个将会造成在计算第三步，也就是temp1的时候，我们会将已经计算好的temp0带入到右边的公式里，这样计算的结果自然和上面的方法不同。
$$
temp0 := \theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)
$$
$$
\theta_0 := temp0
$$
$$
temp1 := \theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)
$$
$$
\theta_1 := temp1
$$
同步更新是梯度下降常用的一种算法，实际上也是更自然的实现方法，当人们谈到梯度下降的时候指的就是同步更新。采用非同步更新时代码也许可以正常运作，但并不是人们所指的梯度下降算法。因此，在梯度下降中真正实现同时更新，就是梯度下降算法的梗概。

同前面学习代价函数一样，我们先分析当只有一个参数时候的情况：

> repeat until convergence{ 
>
> 
>   
> $$
> \theta_1 := \theta_1 - \alpha\frac{d}{d\theta_1}J(\theta_1)
> $$
> }

先画一个粗略的$J(\theta_1)$的图像：

![][4]

很显然，当斜率为负时，$\theta_1$的值在不断减小；当斜率为正时，$\theta_1$的值在不断增加。这里我们要注意，我们需要调整学习速率$\alpha$为一个合适的值，使得梯度下降算法能在合理的时间内收敛。如果太小，那梯度下降的速度会非常慢，如果太大，梯度下降的时候可能直接越过了最小值导致无法收敛或者偏离出去了。

![][5]

接下来我们再继续讨论在具体应用于线性回归时候的形式，我们可以将实际成本函数和实际假设函数替换为:

repeat until convergence{
$$
\theta_0 := \theta_0
-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)
$$
$$
\theta_1 := \theta_1 -\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x_i)-y_i)x_i) 
$$
}

其中，m是训练集的大小，$\theta_0$和$\theta_1$是同步更新的两常量，$x_i,y_i$是训练集中的数据。这里也顺便贴一下第二个等式是怎么计算来的：

![][6]

因此，这是原始的代价函数的梯度下降算法，在每一步的梯度下降中都遍历了训练集的每一个样本，这个被称作批量梯度下降。注意，虽然梯度下降通常容易受到局部最小值的影响，但是我们在这里提出的线性回归的优化问题只有一个全局最优值，没有其他局部最优值，因此梯度下降在全局最小值时总是收敛(假定学习速率α不是太大)。

以上，为梯度下降的基础知识，后续还会有相关的问题讨论，本文笔记主要关联吴恩达机器学习第一周课程中的Parameter Learning部分。

[1]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%9B%BE%E7%89%871.png
[2]: http://www.leafw.cn/wp-content/uploads/2018/08/clip_image001.jpg
[3]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%9B%BE%E7%89%873.png
[4]: http://www.leafw.cn/wp-content/uploads/2018/08/ViewImage.png
[5]: http://www.leafw.cn/wp-content/uploads/2018/08/gradient.png
[6]: http://www.leafw.cn/wp-content/uploads/2018/08/pds.png
