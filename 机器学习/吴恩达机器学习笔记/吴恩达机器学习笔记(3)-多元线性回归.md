之前的文章中已经讲述了简单的一元线性回归的基础知识，现在我们来继续讨论多元线性回归。本文针对吴恩达机器学习第二周课程多元线性回归部分。 

## 假设函数
所谓多元线性回归正如其名字一样，就是说这里的变量是多个的，下面介绍一下如何表示含有多个变量情况下的假设函数：
$$
h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n
$$
之前的一元线性回归中有变量$x^(i)$表示i个训练样本的特征值，现在由于有多个变量，因此我们用$x_j^(i)$表示第i个训练样本的特征j的值，用$x^(i)$表示第i个训练样本的所有特征，m表示训练集的大小，n表示特征的数量。为了更好的理解这个函数，我们可以举个房价的例子：想象$\theta_0$是房子的基础价格，$\theta_1$表示单位面积的房价，$\theta_2$表示每一层的房价，以此类推，则$x_1$表示房子的面积，$x_2$表示房子的楼层数等等。
使用矩阵的定义我们可以更简洁的表达出假设函数，这里我们假设$x_0^i=1(i\in1,2,...m)$,这使得我们可以很方便的进行$\theta,x$的矩阵运算。
$$
h_\theta(x) = \begin{bmatrix} \theta_0 & \theta_1 & ... & \theta_n \end{bmatrix} \begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ ... \\ x_n \end{bmatrix} = = \theta^Tx
$$

## 梯度下降
同样的，针对多远线性回归的梯度下降算法的公式肯定也会不同，我们这里只需要补充上需要重复的n个特征量即可：
repeat until convergence:{
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^(i))-y^(i))x_0^(i)
$$

$$
\theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^(i))-y^(i))x_1^(i)
$$

$$
\theta_2 := \theta_2 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^(i))-y^(i))x_2^(i)
$$
$$
...
$$
$$
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^(i))-y^(i))x_j^(i)\quad j:=0,1...n
$$
}

当然，这里也不要忘了，$\theta_0$，$\theta_1$... $ \theta_j$是同步更新的。

## 特征缩放
那么现在如果有一个问题包含了多个特征，比如说上面的房价问题，我们先假设有两个特征：
$$
x_1 = 房子面积（0-200平米）\\
x_2 = 房间数量（1-5）
$$
那么我们画出的代价函数轮廓图图像可能就是相对比较细长，那么我们通过梯度下降去寻找最小值的时候需要花很长时间才能走到终点，并且可能会来回波动，如下图所示：

![][1]

避免这种情况的方法就是想办法将我们的变量范围缩小到一个合适且相近的范围，一般来说定义到[-1,1]或者[-0.5,0.5]。如何缩小到这个范围呢，我们可以使用下面这个公式：
$$
x_i := \frac{x_i-\mu_i}{s_i}
$$
其中$\mu_i$表示该特征所有值的平均值，$s_i$表示该特征取值范围(最大值减去最小值)或者标准差，在这个例子中假设你的房子面积为110，房间为3，我们可以得出：
$$
x_1 := \frac{110-平均面积(假设为100)}{200} = 0.05\\
x_2 := \frac{3-平均每户房子包含的房间数（假设为2）}{4} = 0.25
$$
两个变量都是在[-0.5,0.5]之间，从而我们画出的轮廓图可能就会相对较圆一点：

![][2]

这样我们梯度下降到最小值点的速度就会相对较快一点。我们这里用的方法就叫做**特征缩放**(Feature Scaling)。

## 学习速率
上一篇我们有说过学习速率的取值会影响到梯度下降的效率和准确性，取值太小会导致梯度下降速度很慢，取值太大会导致梯度下降的时候直接越过最小值导致无法收敛。
$$
\theta_j := \theta_j - \frac{\partial}{\partial\theta_j}J(\theta)
$$
那么我们这里就讲讲如何对α进行取值，首先画出代价函数$J(\theta)$的图像，x轴表示梯度下降算法的迭代步数，注意这里和之前的横轴表达的意思不一样，之前一般是用来表示参数$\theta$,纵轴表示$J(\theta)$的值。简单的说就是这个图像表示了我们梯度下降迭代了多少步后对应的代价函数的值。

![][3]

那么也就是说，如果梯度下降算法是正常工作的，那这个图像就应该是一直处于下降状态的。图中的曲线到400步的时候已经趋于平坦，即已经基本收敛了。因此这条曲线可以用来判断梯度下降算法是否已经收敛。有时候也可以进行一些自动的收敛测试，比如如果发现代价函数的下降小于一个很小的值那么就可以看作为已经收敛如$10^-3$，但选择这个阈值往往只很困难的，因此大部分情况下还是要通过查看这个曲线图来判断是否收敛。
至于梯度下降运行不正确的情况下的图像就可能有如下几种情况：

![1][4]

![2][5]

![3][6]
第一张图图像是一直往上升的方向去的，明显是错误的运行状况。第二张和第三张说明你的α取值偏大了一点，需要取小一点试试。对于真正的有效的α，$J(\theta)$应当每一步都在递减，但如果α太小了，之前说过，梯度下降的速度会很慢。

## 多项式回归
以上我们已经大致了解了多项式回归，那这里我们先谈谈如何选择特征。比如说上面的房子问题，我们可以选房子的长和宽当做两个特征$x_1,x_2$来看，但这只是一种想法，其实我们也可以把房子的面积$x_1*x_2$来当做特征，这样我们只使用了这一个特征，这取决于你从什么样的角度去审视这个问题，通过定义一个新的特征，我们有时候可能会得到一个更好的模型。
与选择特征相关的一个概念，被称作多项式回归（polynomial regression）。我们还是用房价的例子，我们一开始用的可能是一次函数去模拟这个数据集的情况，但尝试了之后发现可能一次函数不足以模拟出实际情况。二次函数很显然不行，房价会有下降的趋势，房价不可能达到一定高度后又下降回来，那这里就可以尝试使用三次函数，比如：
$$
h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x_1^3
$$
这里第一个特征$x_1$设为房子的面积，第二个特征为房子面积的平方$x_2 = x_1^2$，第三个为房子面积的立方$x_3 = x_1^3$。最终将这个三次函数拟合到我们的数据上。但我们现在用这样的方式选择特征的话,三个特征的范围的差距就会很大，假设房子面积[1,100]，那么$x_1\in[1,100],x_2\in[1,100^2],x_3\in[1,100^3]$。那么如果使用梯度下降法的话，特征值的归一化就很重要，即将不同类型的特征数值大小变为一致。那这里就不适用三次函数的模型，我们试试平方根函数，我们可以假设模型为$h_\theta(x) = \theta_0+\theta_1x_1+\theta_2\sqrt x_1$,平方根函数也是一个递增的函数，而且递增速度会越来越趋于平缓，可能与我们的数据更为契合。

![][7]

可能在选择特征的这个问题上经常难以抉择，再后面的内容中我们会继续讨论一些算法，他们能自动选择要使用什么特征，因此可以使用某一个算法观察数据，决定最终使用什么函数。

  [1]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180808225014.png
  [2]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180808230513.png
  [3]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180808233154.png
  [4]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180808233545.png
  [5]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180808233626.png
  [6]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180808233618.png
  [7]: http://www.leafw.cn/wp-content/uploads/2018/08/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180809221226.png
