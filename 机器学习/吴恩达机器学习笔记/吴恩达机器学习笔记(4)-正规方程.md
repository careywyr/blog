在前面的学习中，我们一直使用的线性回归算法是梯度下降法，即为了最小化代价函数$J(\theta)$，我们需要通过迭代很多步来收敛到全局最小值。那么现在我们来学习一种新的解法来求解。即正规方程。正规方程提供了一种求$\theta$的解析解法，相比较迭代算法，我们可以直接一次性的求解出$\theta$的最优值。

我们首先举一个例子来帮助理解正规方程，我们假设有一个非常简单的代价函数$J(\theta)$，它就是一个实数$\theta$的函数：$J(\theta) = a\theta^2+b\theta+c$。所以现在假设数$\theta$只是一个标量，或者说数$\theta$只有一行，是一个数字不是向量。假设我们的代价函数 J 是这个实参数数$\theta$的二次函数，那么如何最小化一个二次函数呢。很常见的办法就是对函数求导，并导数为 0，即求解$J''(\theta)=0$，这样就可以求得使得$J(\theta)$最小时$\theta$的值。但实际情况中，我们的数$\theta$往往不是一个实数，而是一个 n+1 维的向量，并且代价函数 J 是这个向量的函数，也就是：$J(\theta_0,\theta_1...\theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^(i))-y^(i))^2$。那么如何最小化这个函数呢，微积分里我们学过偏导，即这里我们要对每个参数$\theta$求 J 的偏导数，但这个求导会非常复杂，因此实际上并不会真的对这个函数进行遍历所有的偏微分，这里只是告诉大家想要实现这个过程所需要知道的内容。

现在我们看看之前用过的房价的例子,数据集大小 m=4，我们在额外的加上一列对应特征向量$x_0$,这一列取值永远为 1:
![](https://upload-images.jianshu.io/upload_images/1537405-288583a100fc47f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

接下来我们就可以构建一个矩阵 X，这个矩阵包含了训练样本的所有特征变量，X 是个 m\*(n+1)维矩阵。对 y，也就是我们预测的值，可以构建一个向量，y 是个 m 维度的向量.m 是训练样本数量, n 是特征变量数：

$$
X = \begin{bmatrix}
    1 && 2104 && 5 && 1 && 45 \\
    1 && 1416 && 3 && 2 && 40 \\
    1 && 1534 && 3 && 2 && 30 \\
    1 && 852 && 2 && 1 && 36
    \end{bmatrix}
$$

$$
Y = \begin{bmatrix}
    460 \\
    232 \\
    315 \\
    178
    \end{bmatrix}
$$

如果我们用矩阵 X 和向量 Y 来计算的话，那么最终的结果如下等式：

$$
\theta = (X^TX)^{-1} X^Ty
$$

假设我们有 m 个训练样本$x^(1),y^(1)$直到$x^(m),y^(m)$,特征变量数目为 n,因此每个训练样本$X^(i)$可以看作一个向量：

$$
x^(i) = \begin{bmatrix}
        x_0^(i)\\
        x_1^(i)\\
        x_2^(i)\\
        ...\\
        x_n^(i)\\
        \end{bmatrix}
$$

接下来我们要构建矩阵 X，构建矩阵 X 的方法也被称作设计矩阵。每个训练样本给出一个这样的特征向量，取第一个训练样本，也就是一个向量，取它的转置，最后是扁长的样子。让$x_1^T$作为我设计矩阵的第一行 然后第二个训练样本$x_2$进行转置 让它作为 X 的第二行。以此类推，直到最后一个训练样本取它的转置作为矩阵 X 的最后一行，这样矩阵 X 就是一个 m\*(n+1) 维矩阵：

$$
X = \begin{bmatrix}
    ... &&  (x^(1))^T && ...\\
    ... &&  (x^(2))^T && ...\\
    ... &&  (x^(3))^T && ...\\
    ...\\
    ... &&  (x^(m))^T && ...\\
    \end{bmatrix}
$$

举个例子，假设我们只有一个特征变量，就是说除了$x_0$之外只有一个特征变量,而$x_0$始终为 1,所以如果我的特征向量$x_i$等于 1,也就是$x_0$和某个实际的特征变量$x_i$等于 1,

$$
x^(i) = \begin{bmatrix}
        1 \\
        x^(i)
        \end{bmatrix}
$$

那么按照上述的方法，设计矩阵就是这样：

$$
X = \begin{bmatrix}
        1 && x_1^(1)\\
        1 && x_2^(1) \\
        ... \\
        1 && x_m^(1)
    \end{bmatrix}
$$

这样，就会是一个 m\*2 维矩阵，而这里的向量 y，则是一个 m 维向量：

$$
y = \begin{bmatrix}
        y^(1) \\
        y^(2) \\
        ... \\
        y^(m)
    \end{bmatrix}
$$

这样我们得到了矩阵 X 和向量 Y 的结果，因此就可以使用正规方程来求解$\theta$了。

$$
\theta = (X^TX)^{-1} X^Ty
$$

之前我们在梯度下降的时候说过特征变量的特征缩放，即将所有的特征值缩小到类似的范围内。而如果使用正规方程的话，就不需要再进行特征缩放的操作了，但如果使用梯度下降法，就要根据不同特征的范围来判断是否需要使用特征缩放。
特征方程看起来相对于梯度下降要简单很多，但也不是万能的，这里我们列出这两个的特点来进行对比一下：
![][1]

（懒得手写了就直接赋值吴恩达老师的图了==），这里就提一下两者的最后一个特点。由于矩阵 X 是 m\*(n+1)维的矩阵，当我们的特征值非常多的时候，计算$(X^TX)^{-1}$时候的计算量是非常庞大的，因此计算的速度会非常慢，但 n 的大小很显然跟梯度下降的速度没关系，所以梯度下降算法依然可以正常运行。这里吴恩达老师是建议在 n=10000 的时候就考虑一下是否要使用梯度下降法了，10000 以下都可以使用正规方程，当然这里还是需要通过实际情况来决定。

最后也稍微说一点关于如果矩阵$X^TX$不可逆怎么办。出现这种情况的可能性大概有两种，一种是特征选的很多余了，比如选了房子的平方面积作为特征还选了房子的英尺计算的面积，这样就明显很多余了；还有一种是特征选的太多，比如你有 10 个样本，结果选了 1000 个特征，这显然也不适合，可以适当的删除一些特征或者以后会提到的正则化的线代方法。

以上，就是关于正规方程的一些简单介绍，针对吴恩达机器学习的正规方程那一章节。

[1]: https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/%E6%AD%A3%E8%A7%84.png
