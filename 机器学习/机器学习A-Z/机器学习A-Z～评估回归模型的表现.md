# 机器学习A-Z～评估回归模型的表现

本文开始讲解关于如何评估回归模型的表现的几个方式。

## R平方

首先来讲一下前面也有提到的R平方的概念。来看下面这个例子。下面红色的是数据对应的点，黑色的直线是我们拟合出来的一条简单线性线性回归。

![](https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-03%20%E4%B8%8B%E5%8D%8812.39.19.png)

怎么拟合这条直线呢？实际上就是假设平面上有一条直线，我们将这些数据点向这条直线上做投影，那么这些投影和实际值的差的平方和最小时就能得到这条直线。这个也有个专业术语叫做剩余平方和（$SS_{res}$）。

接下来还有一种看待数据的方式，假设定义$y_{avg}$即y的平均值，如图中黑色直线，然后表示出所有数据点，那么这个时候也可以同样的将所有点对这条线做投影，此时将这些点和平均值的差值平方和叫作共平方和（$SS_{tot}$）。

![](https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-02-03%20%E4%B8%8B%E5%8D%8812.47.05.png)

这个表达方式相对于剩余平方和不是很常见，但这个项和这些数据的方差有很大关系，因为方差其实就是这里平方和的前面乘以一个系数。

定义完了剩余平方和和共平方和后我们就可以定义要说的R平方。如下所示：
$$
 R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$
首先我们看看这个共平方和，实际上和拟合出来的线性回归模型是没有什么关系的，因为只要有这些数据点，那这个共平方和是永远存在的。那么会不会出现共平方和为0呢？在实际情况下一般是不存在的，因为那所有数据是一样的。其实图中这条水平直线也是线性回归模型的一种，这种方式就是以所有因变量的平均值来进行模拟，不考虑自变量的变化。共平方和也是剩余平方和的一种。当这个剩余平方和越小，那么这里的R平方越大，即模型越好，R平方最大就为1。那么什么时候R平方会为负的呢？很简单其实就是剩余平方和比共平方和还要大，但这种情况也是基本不会出现。

## 广义R平方（Adjusted $R^2$）

对于上述所说的R平方也是存在一定的局限性。现在来更深入的谈谈这个R平方。因为刚刚所说的R平方所针对的实际上是简单线性回归。如果说此时有个多元线性回归模型$y = b_0 + b_1 * x_1 + b_2 * x_2$，R平方会发生什么样的变化？

对于多元线性回归，R平方的定义还是类似的，依然是个最优化问题。我们需要寻找$b_0,b_1,b_2$这些参数，来使得其剩余平方和最小。我们知道R平方是越大越好，那这里就有个核心的问题。假设这里有个线性回归模型是两个自变量的，此时再给它添加一个新的自变量，R平方会发生什么变化？这里先讲一个结论：**R平方永远不会降低**。也就是说，当我们新加一个自变量，不管这个自变量是什么，我们模型的拟合效果只会越来越好。这里给个简单的证明：

假如说可以找到一个b3可以使得拟合效果更好，即使得剩余平方和更小，由于共平方和是固定的，所以整个的R平方会升高。那么假设说我们尝试了很多的b3都无法使得剩余平方和更小，但有个最差的选择，就是b3为0。此时就相当于没有加上这个b3，R平方自然不会降低。

用之前说提过的薪水的例子来说，这里的自变量可以包括工作年限和职位，此时如果再加入一个手机尾号来做为新的自变量，当然从直观的感觉上都知道这个跟薪水没什么关系。但从数据上而言，是能找到一丝关联性的，因此这时的b3会很接近于0但不会是0。此时的R平方会更大。但这时能说这个模型就是拟合的更好么？显然是不合理的。这时就看出了R平方的局限性。需要用广义的R平方来进行评估：
$$
Adj R^2 = 1 - (1 - R^2) * \frac{n-1}{n-p-1}
$$
这里p是指number of regressors（自变量个数）,n是指sample size（数据个数）。n由于是数据集的大小，因此跟模型没什么关系，但这里的p对于拟合的模型实际上有一种惩罚的效果，当p升高时，这里的分母就会降低，那么这个分数项就会升高，而此时R平方又会升高，那么$1-R^2$就会降低，此时实际上就有一种类似拔河的感觉，两个乘数一个升高一个降低，我们就不知道这个广义R平方到底会降低还是升高。这就要看这个新加入的自变量对于模型的精准度是否有帮助，能不能弥补后面的惩罚项。这个广义R平方就能来评估多元线性回归模型的好坏的一个很好的参数。这时回过头看之前的一个多元线性回归模型的拟合结果会发现有不同的看法了。这里就不去赘述，可以自己去尝试。

因为这里的R平方是个比较客观的数值来评判模型的好坏，而不是自己主观定义的比如之前的p值。因此R平方能更好的反映出一个模型的好坏。但是不是说之前说的p值反向淘汰算法等等没有意义了，因为这个反向淘汰给予了我们一个顺序来一步步的删除自变量中的一个，直到达到了最好的模型，这也就我们比较不同模型之间的广义R平方的最好的条件。  

## 回归模型系数的含义

现在再来讲讲多元线性回归中各个参数代表着什么样的意义。继续说之前的公司年收入的例子。首先看看这些参数的符号代表的意义。这个其实很明显，比如如果所有参数符号都是正的，很明显如果自变量越大那么因变量就会越大。

接下来看参数的大小，比如自变量有市场营销的开支和研发的开支。如果研发的开支的系数是市场营销的20倍，是否能说研发的开支对公司年收入的影响要远远大于市场营销的开支带来的影响呢？但这里不能这样随便的下结论，因为参数的大小没有很显然的影响。假设这里研发的参数值时0.7379，研发开支的单位是美元，那如果用万美元来做单位，对于现在的模型，这里的参数则要变成7379，此时显然不能说现在的研发开支相对于之前的研发开支的影响力要大很多。因此这里系数的大小和其自变量的大小也是息息相关的。但我们可以做出的结论是：**在单位自变量的变化下，这个参数代表着相应因变量会有多少变化**。而且在更多的情况下，不同的自变量的单位也是不同的，因此这里要注意是单位自变量的变化下。但如果两个自变量的单位是相同的，比如这里的研发开支和市场营销的单位就是一样的。也就是说花一美元在研发上相当于在市场营销上要花20美元。对于投资人，这条结论就是很有意义的，他可能就会投资更多的钱在于研发上。

再说说不同模型中，同一个自变量对应的参数很可能是不同的，它所对应的系数不仅仅和自变量本身有关，也和同一个线性回归模型中其他的系数存在与否，数据怎么样也是有关系的。因为这每个参数代表着**在某一个多元线性回归模型下，如果其他自变量不变，只对这一个自变量进行改变，那么它对预测的结果所带来的影响**。

以上，就是关于如何评估线性回归模型的介绍。